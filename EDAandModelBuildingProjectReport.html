<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=wJuDEFlUsDZt7lpAAPdLm9zcEnadfPiiWrsKixWiRBLOfsfM6rvuuu7h1pY3r_-A');ul.lst-kix_lqqrwyjgn6iu-8{list-style-type:none}.lst-kix_lqqrwyjgn6iu-0>li:before{content:"\0025cf  "}.lst-kix_lqqrwyjgn6iu-1>li:before{content:"\0025cb  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_lqqrwyjgn6iu-7>li:before{content:"\0025cb  "}.lst-kix_lqqrwyjgn6iu-6>li:before{content:"\0025cf  "}.lst-kix_lqqrwyjgn6iu-8>li:before{content:"\0025a0  "}.lst-kix_lqqrwyjgn6iu-5>li:before{content:"\0025a0  "}ul.lst-kix_lqqrwyjgn6iu-2{list-style-type:none}ul.lst-kix_lqqrwyjgn6iu-3{list-style-type:none}ul.lst-kix_lqqrwyjgn6iu-0{list-style-type:none}ul.lst-kix_lqqrwyjgn6iu-1{list-style-type:none}.lst-kix_lqqrwyjgn6iu-2>li:before{content:"\0025a0  "}.lst-kix_lqqrwyjgn6iu-4>li:before{content:"\0025cb  "}ul.lst-kix_lqqrwyjgn6iu-6{list-style-type:none}ul.lst-kix_lqqrwyjgn6iu-7{list-style-type:none}ul.lst-kix_lqqrwyjgn6iu-4{list-style-type:none}.lst-kix_lqqrwyjgn6iu-3>li:before{content:"\0025cf  "}ul.lst-kix_lqqrwyjgn6iu-5{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c10{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:168.8pt;border-top-color:#000000;border-bottom-style:solid}.c8{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:74.1pt;border-top-color:#000000;border-bottom-style:solid}.c19{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:16pt;font-family:"Cambria Math";font-style:normal}.c17{padding-top:0pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center;height:12pt}.c5{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:right;height:12pt}.c23{padding-top:0pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c12{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:center}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c9{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c11{border-spacing:0;border-collapse:collapse;margin-right:auto}.c6{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c15{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{font-weight:700;font-size:14pt;font-family:"Cambria Math"}.c2{font-size:12pt;font-family:"Cambria Math";font-weight:400}.c14{font-weight:700;font-size:12pt;font-family:"Cambria Math"}.c24{font-weight:400;font-size:26pt;font-family:"Cambria Math"}.c21{margin-left:36pt;text-indent:-36pt}.c7{padding:0;margin:0}.c25{font-style:italic}.c22{height:16pt}.c3{height:0pt}.c20{height:12pt}.c16{text-indent:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:0pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:center}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Cambria Math"}p{margin:0;color:#000000;font-size:12pt;font-family:"Cambria Math"}h1{padding-top:0pt;-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;font-size:16pt;padding-bottom:0pt;line-height:2.0;page-break-after:avoid;text-decoration-skip-ink:none;font-family:"Cambria Math";orphans:2;widows:2;text-align:left}h2{padding-top:0pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:0pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Cambria Math";line-height:2.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c15 doc-content"><div><p class="c18"><span class="c6 c2"></span></p></div><p class="c17 title" id="h.rvzhbb17bpnf"><span class="c6 c24"></span></p><p class="c23 title" id="h.xy3zuw47c0d"><span>Predicting Loan Repayment<br>with the LendingClub Dataset</span></p><p class="c9 c20"><span class="c6 c2"></span></p><p class="c9"><span class="c6 c2">Abstract:</span></p><p class="c9"><span class="c6 c2">The LendingClub data set features 27 variables and 396,030 rows with a variety of information related to loans and those who use them. This data was used to build a number of statistical models, including logistic regression and several tree-based models, to predict whether or not a given loan would be repaid. The models were able to obtain approximately 60-65% accuracy with Kappa values hovering around 28%, and no one model performed significantly better than another. Given the large sample size, the models were trained on just 1% of the data, which may have had negative implications for their applicability and results. Despite this, the models were able to predict when loans would be repaid at a better rate than random guessing. With finer tuning and better hardware, this work could be applied to the industry to help creditors determine who is best suited for a loan before it is issued. </span></p><h1 class="c0 c22" id="h.gfx6yj2tmntu"><span class="c19"></span></h1><p class="c9 c20"><span class="c6 c2"></span></p><h1 class="c0" id="h.wvufrdalqqu2"><span class="c19">Introduction</span></h1><h2 class="c0" id="h.21y8sfmiq2aa"><span class="c6 c13">Problem and Goal</span></h2><p class="c9 c16"><span class="c6 c2">It can often be difficult to determine whether or not a candidate is suitable for a loan given many different factors and variables that may differ slightly from past candidates. Given historical data from LendingClub, a US peer-to-peer lending company, on loans from their company, we can analyze this data to build a model that will predict whether or not a given loan will be repaid by the borrower. This will streamline the screening process of new loan applicants, who, given the necessary information, will be accepted or rejected for a loan. </span></p><p class="c9 c16"><span class="c6 c2">LendingClub is a peer-to-peer lending platform. This means that LendingClub facilitated the process of an individual providing a loan to another individual through its website. This is different from a bank loan since there is no bank or financial institution involved in providing the actual loan money (Forbes).</span></p><p class="c9 c16"><span>Peer-to-peer lending is relatively new, starting up in 2005. Applicants who receive loans tend to get better rates than those who get their loans from a bank. Additionally, investors who provide the capital for the loans are able to get more return on their investment than if that money were sitting in a savings account (Kagan).</span></p><h2 class="c0" id="h.8khn0h36m4mi"><span>Feature </span><span class="c6 c13">Descriptions</span></h2><p class="c9"><span class="c6 c2">This dataset has 27 variables:</span></p><ul class="c7 lst-kix_lqqrwyjgn6iu-0 start"><li class="c5 li-bullet-0"><span class="c2">loan_amnt</span><span>: </span><span class="c2">The listed loan amount applied for by the borrower</span><span>. It is </span><span class="c2">current in the sense that</span><span>&nbsp;</span><span class="c6 c2">if the loan is reduced by the company, this amount will be updated</span></li><li class="c5 li-bullet-0"><span class="c2">term</span><span>: The n</span><span class="c2">umber of loan payments in months</span><span>&nbsp;(36 or 60 months)</span></li><li class="c5 li-bullet-0"><span class="c2">int_rate</span><span>:</span><span class="c6 c2">&nbsp;Interest rate of the loan</span></li><li class="c5 li-bullet-0"><span class="c2">installment</span><span>:</span><span class="c6 c2">&nbsp;Monthly payment owed by borrower if loan originates</span></li><li class="c5 li-bullet-0"><span class="c2">grade</span><span>:</span><span class="c2">&nbsp;</span><span>LendingClub </span><span class="c6 c2">assigned loan grade</span></li><li class="c5 li-bullet-0"><span class="c2">sub_grade</span><span>:</span><span class="c2">&nbsp;</span><span>LendingClub </span><span class="c6 c2">assigned loan subgrade</span></li><li class="c5 li-bullet-0"><span class="c2">emp_title</span><span>:</span><span class="c6 c2">&nbsp;Job title of borrower when applying for loan</span></li><li class="c5 li-bullet-0"><span class="c2">emp_length</span><span>:</span><span class="c6 c2">&nbsp;Employment length in years</span></li><li class="c5 li-bullet-0"><span class="c2">home_ownership</span><span>:</span><span class="c6 c2">&nbsp;Home ownership status of borrower</span></li><li class="c5 li-bullet-0"><span class="c2">annual_inc</span><span>:</span><span class="c6 c2">&nbsp;Self-reported annual income of borrower during registration</span></li><li class="c5 li-bullet-0"><span class="c2">verification_status</span><span>:</span><span class="c2">&nbsp;Indicates whether the income was verified by the </span><span>LendingClub</span><span class="c6 c2">, not verified, or verified by the income source</span></li><li class="c5 li-bullet-0"><span class="c2">issue_d</span><span>: </span><span class="c2">The month and year</span><span>&nbsp;</span><span class="c6 c2">the loan was funded</span></li><li class="c5 li-bullet-0"><span class="c14">loan_status</span><span>:</span><span class="c2">&nbsp;Current loan status</span><span>&nbsp;</span><span class="c6 c2">*This is our response variable*</span></li><li class="c5 li-bullet-0"><span class="c2">purpose</span><span>:</span><span class="c2">&nbsp;Category provided by borrower in the loan request </span><span>describing</span><span class="c2">&nbsp;what the </span><span>money from the loan will be used for</span></li><li class="c5 li-bullet-0"><span class="c2">title</span><span>:</span><span class="c6 c2">&nbsp;The loan title, provided by the borrower</span></li><li class="c5 li-bullet-0"><span class="c2">dti</span><span>:</span><span class="c2">&nbsp;A ratio calculated by using the borrower&rsquo;s total monthly debt payments on the total debt obligations, excluding mortgage and requested </span><span>LendingClub </span><span class="c6 c2">loan, divided by the borrower&rsquo;s reported monthly income</span></li><li class="c5 li-bullet-0"><span class="c2">earliest_cr_line</span><span>:</span><span class="c6 c2">&nbsp;The month and year the borrower&rsquo;s earliest reported credit line was opened</span></li><li class="c5 li-bullet-0"><span class="c2">open_acc</span><span>:</span><span class="c6 c2">&nbsp;The borrower&rsquo;s number of open credit lines </span></li><li class="c5 li-bullet-0"><span class="c2">pub_rec</span><span>:</span><span class="c6 c2">&nbsp;The borrower&rsquo;s number of derogatory public records</span></li><li class="c5 li-bullet-0"><span class="c2">revol_bal</span><span>:</span><span class="c6 c2">&nbsp;The total credit revolving balance</span></li><li class="c5 li-bullet-0"><span class="c2">revol_util</span><span>:</span><span class="c6 c2">&nbsp;The revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit</span></li><li class="c5 li-bullet-0"><span class="c2">total_acc</span><span>:</span><span class="c6 c2">&nbsp;The borrower&rsquo;s total number of credit lines </span></li><li class="c5 li-bullet-0"><span class="c2">initial_list_status</span><span>:</span><span class="c2">&nbsp;The initial status of the loan, applicable values being</span><span>&nbsp;</span><span class="c6 c2">W (whole) or F (fractional)</span></li><li class="c5 li-bullet-0"><span class="c2">application_type</span><span>:</span><span class="c2">&nbsp;Indicates whether the loan is an individual application, a joint application with two co-borrowers, or a </span><span>direct pay</span></li><li class="c5 li-bullet-0"><span class="c2">mort_acc</span><span>:</span><span class="c6 c2">&nbsp;The number of mortgage accounts</span></li><li class="c5 li-bullet-0"><span class="c2">pub_rec_bankruptcies</span><span>:</span><span class="c6 c2">&nbsp;The number of public record bankruptcies</span></li><li class="c5 li-bullet-0"><span class="c2">address</span><span>:</span><span class="c6 c2">&nbsp;The address of the borrower, provided in the loan application</span></li></ul><h1 class="c0" id="h.tzo61e15wa2n"><span class="c19">Data Exploration</span></h1><p class="c9 c16"><span class="c6 c2">The data exploration in our project served to provide an understanding of the variables in the dataset. It played a large role in informing how we would preprocess the data and prepare it for model fitting. We developed an understanding of the distributions of the variables, their correlations with each other, and their correlations with the target variable. Our full data exploration is shown in the R markdown file, but we discuss our notable findings here.</span></p><h2 class="c0" id="h.6goqcbyupexo"><span class="c6 c13">Feature Distributions</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our first important finding was understanding the grade and subgrade variables. These variables were indicators of the risk associated with a loan, and would likely be a good indicator of whether or not a loan would be charged off. We found that loan grade ranged from &lsquo;A&rsquo; to &lsquo;G&rsquo;, with &lsquo;A&rsquo; being the least risky loan, and &lsquo;G&rsquo; being the most risky loan. We plotted the distribution of this variable and found that grades &lsquo;B&rsquo; and &lsquo;C&rsquo; were the most common in the dataset. The riskier grades, such as &lsquo;E&rsquo;, &lsquo;F&rsquo;, and &lsquo;G&rsquo; were less common. </span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also found that loan subgrade was similar to loan grade in many ways. Loan subgrade breaks down each letter grade into five subgroups, numbered 1 through 5. Loans with subgrade 1 are less risky than loans in subgrade 5 in the same letter grade. The distribution of subgrade is the same shape as the distribution of grade, just at a finer resolution since there are more categories in subgrade. Subgrade &lsquo;B3&rsquo; was the most common, and subgrades starting with &lsquo;E, &lsquo;F&rsquo;, and &lsquo;G&rsquo; were the least common. With the seven letter grades and five subgrades within each grade, this variable has 35 possible values.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 418.00px; height: 281.05px;"><img alt="" src="images/image8.png" style="width: 457.63px; height: 312.54px; margin-left: -16.04px; margin-top: -5.17px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.50px; height: 289.93px;"><img alt="" src="images/image4.png" style="width: 458.26px; height: 326.66px; margin-left: -17.99px; margin-top: -9.23px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also found a number of categorical variables that had a very high number of unique values. These variables were likely open-ended questions on the loan application, and the results have not been grouped nor standardized in any way. Employee title, which provides a description of the loan applicant&rsquo;s job, had 173,106 unique values. Loan description had 48,818 unique values. Address, which provided the home address of the loan applicant, had 393,700 unique values.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another important finding was the distribution of home ownership status. We found that there were six unique values in this column, but only three values were substantially used. The overwhelming majority either owned, rented, or had a mortgage on their home. The three other categories were &lsquo;any&rsquo;, &lsquo;none&rsquo;, and &lsquo;other&rsquo;, and these three are rarely seen in the dataset.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 443.50px; height: 303.05px;"><img alt="" src="images/image1.png" style="width: 476.32px; height: 339.69px; margin-left: -12.98px; margin-top: -7.84px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c16"><span class="c6 c2">Application type is another variable that is unbalanced. The three values in this column were direct pay, individual, and joint. We plotted the distribution of this variable and found that individual applications were overwhelmingly predominant. The other types of applications were almost never used.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 425.00px; height: 292.70px;"><img alt="" src="images/image3.png" style="width: 453.33px; height: 324.01px; margin-left: -13.80px; margin-top: -5.82px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We examined the distribution of our target variable, loan status, which is a binary variable that indicates whether or not the loan applicant successfully paid off their loan. We found that the target variable was unbalanced. There are significantly more instances of loans paid off than loans charged off. Sometimes models struggle with an unbalanced target variable, and they are generally better at identifying the majority class in that case.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 393.16px; height: 268.50px;"><img alt="" src="images/image9.png" style="width: 423.72px; height: 302.49px; margin-left: -15.62px; margin-top: -8.16px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c0" id="h.dt8jc6v5p9b0"><span class="c6 c13">Feature Correlations</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We began our investigation of feature correlations with a correlation matrix amongst the quantitative variables in our dataset. We found that total accounts and open accounts were very similar variables, having a correlation of 0.678. Similarly, the number of public derogatory records and the number of public bankruptcies had a correlation of 0.695. The most highly correlated variables were the loan amount and the monthly installment, correlated at 0.955. This makes sense because the monthly payment someone needs to make on a loan is dependent on the amount of money they are borrowing, the amount of time they have to pay it back, and the interest rate.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We then began investigating the correlation of variables with the target variable. As expected, loan grade had a clear relationship with loan status. Loans with riskier grades, such as &lsquo;F&rsquo; and &lsquo;G&rsquo;, were more likely to be charged off than those with less risky grades.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 437.00px; height: 282.88px;"><img alt="" src="images/image6.png" style="width: 471.78px; height: 335.68px; margin-left: -14.37px; margin-top: -25.65px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c16"><span class="c6 c2">We found that the number of years of employment was most commonly +10 years. The bar chart below shows the distribution of loans charged off and loans fully paid for each possible value in the years of employment column. The proportion of loans fully paid to loans charged off was very similar for each of the categories, indicating that this variable may not be very predictive.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 448.50px; height: 278.86px;"><img alt="" src="images/image2.png" style="width: 483.36px; height: 344.70px; margin-left: -15.49px; margin-top: -38.73px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also examined how loan status varied across the types of home ownership. Those who rented had a slightly higher probability of having their loan charged off, as opposed to those who owned or mortgaged their home.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 433.50px; height: 283.99px;"><img alt="" src="images/image10.png" style="width: 463.99px; height: 330.83px; margin-left: -15.61px; margin-top: -23.15px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We found that those who did not have their income verified were slightly less likely to have their loan charged off. Those who had their income verified by LendingClub or their income source had about the same probability of having their loan charged off. </span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 428.50px; height: 273.19px;"><img alt="" src="images/image7.png" style="width: 463.00px; height: 330.35px; margin-left: -17.07px; margin-top: -33.41px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally, we found that the interest rate on the loan had a noticeable relationship with the loan status. The boxplot below shows the distribution of interest rate, separated by the loan status, and loans that were charged off tended to have a higher interest rate than those that were fully paid.</span></p><p class="c12"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 454.50px; height: 296.82px;"><img alt="" src="images/image5.png" style="width: 454.50px; height: 323.73px; margin-left: 0.00px; margin-top: -26.92px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c0" id="h.arwhb2yi1pzp"><span class="c19">Data Preprocessing</span></h1><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Only three of the given variables had any missing variables: revol_util, mort_acc, and pub_rec_bankruptcies. Mort_acc had significantly more missing values than the other two at 37,795. For all three variables, we decided to impute the missing variables using the median value. This had no effect on the summary statistics of revol_util and pub_rec_bankrupcies, and the only difference in mort_acc was the mean decreasing from 1.81 to 1.736, which was not significant enough to be alarming.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We then considered which variables to remove. Using the visualizations </span><span>from the</span><span class="c2">&nbsp;data exploration, we </span><span>justified the removal of several </span><span class="c2">unnecessary variables. &nbsp;We first decided to remove address, title, and emp_title because they were </span><span>qualitative </span><span class="c2">variables with thousands of unique values, which would be computationally expensive to include and would have likely provided little insight. We removed sub_grade because we felt th</span><span>at</span><span class="c2">&nbsp;just using grade, which already has 7 different categories, would be sufficient in this case. We also removed employment length and issue_d because we did not find them relevant to predicting loan repayment. Application type also did not seem relevant and was extremely unbalanced, so we removed that as well. After comparing the boxplots between open_acc and total_acc, pub_rec and pub_rec_bankruptcies, and revol_util and revol_bal, we kept the one from each of tho</span><span>se pairs </span><span class="c2">that was more balanced between </span><span>the two loan statuses</span><span class="c2">&nbsp;and that seemed more relevant for prediction. Finally, we removed the purpose variable due to the high number of categories, which would be computationally expensive to run if </span><span>we were to create several </span><span class="c2">dummy variables </span><span>to replace it</span><span class="c6 c2">.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Next, we had to convert the remaining categorical variables into dummy variables in order to include them in the model fitting. These variables were: term, grade, home_ownership, home_verification_status, and initial_list_status. Term only had two categories, so we created one dummy variable that transformed the entries into 0s and 1s. </span><span class="c2">Grade</span><span class="c2">&nbsp;had options A-G, so we consolidated this into three different </span><span>variables:</span><span class="c2">&nbsp;A&amp;B, C,D,&amp;E, and F&amp;G, which could also be considered &lsquo;good,&rsquo; &lsquo;fair,&rsquo; and &lsquo;poor&rsquo; grades. Home_ownership included six different categories, but &ldquo;</span><span>any</span><span class="c2">,&rdquo; &ldquo;</span><span>none</span><span class="c2">,&rdquo; and &ldquo;</span><span>other</span><span class="c2">&rdquo; had very little data, so we decided to combine these with &ldquo;</span><span>mortgage</span><span class="c2">&rdquo;, then have &ldquo;</span><span>own</span><span class="c2">&rdquo; and &ldquo;</span><span>rent</span><span class="c2">&rdquo; as their own dummy variables. </span><span>Income </span><span class="c2">verification</span><span>&nbsp;</span><span class="c2">status was </span><span>transformed </span><span class="c2">into two </span><span>dummy variables</span><span class="c2">: verified (which included &ldquo;</span><span>v</span><span class="c2">erified&rdquo; and &ldquo;</span><span>s</span><span class="c2">ource </span><span>v</span><span class="c6 c2">erified&rdquo;) and not verified. Finally, initial_list_status had two categories, &ldquo;w&rdquo; and &ldquo;f,&rdquo; so this was put into one dummy variable. </span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One final variable we had to consider was earlist_cr_line. We felt this variable could be significant for the models, but it was classified as a character variable in month-year format. To transform this variable into </span><span>a quantitative variable</span><span class="c2">, we removed the month and only </span><span>kept </span><span class="c2">the year (the last four </span><span>characters in the column</span><span class="c2">) to </span><span>allow the model to determine </span><span class="c2">how </span><span>different</span><span class="c2">&nbsp;years </span><span>may affect </span><span class="c6 c2">the probability of repayment. </span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The model training process was extremely long when using the hundreds of thousands of records available to us. Additionally, our target variable was unbalanced. Thus, we created a smaller dataset by gathering all of the rows with loans charged off and then gathering a sample of the same size from rows with loans fully paid. This created a smaller, balanced dataset from which to train and test our models. The more balanced dataset also made sensitivity and specificity measures more reasonable, and the accuracy metric in the confusion matrix became a fairer assessment of the model&rsquo;s true skill.</span></p><p class="c9"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Even with this smaller dataset, we still had tens of thousands of records, and model fitting was taking longer than 10 minutes for a single model in some cases. Thus, we created a training set that was only 1,553 records. Even then, some of our models still take several minutes to complete their training. The results of our eight fitted models are based on this small training dataset.</span></p><h1 class="c0" id="h.6hw7nv59qc4p"><span class="c19">Model Fitting</span></h1><h2 class="c0" id="h.lmcvy8cmf0r1"><span class="c6 c13">Logistic Regression</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic regression is a statistical model that takes in a categorical response variable. It is more equipped for predicting classification datasets than linear regression because the model will predict the result as either 0 or 1, rather than somewhere in the middle (which is unhelpful for classification). We specifically studied binary logistic regression, which was applied to this dataset. </span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>T</span><span class="c2">he resulting accuracy was </span><span>64.37</span><span class="c2">%. The sensitivity was </span><span>69.79</span><span class="c2">%, the specificity was </span><span>58.95</span><span class="c2">%, and the Kappa value was </span><span>28.74</span><span class="c6 c2">%. As stated previously, the accuracy decreased when the sample size was reduced, but the sensitivity increased significantly, creating more balanced data. The Kappa value implies slight agreement. </span></p><h2 class="c0" id="h.imy713j54ss1"><span class="c6 c13">Linear Discriminant Analysis</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linear discriminant analysis is a supervised algorithm that can be used for classification. It finds a linear combination of features that creates an optimal separation between classes in the data. </span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The LDA model we created reported an accuracy of </span><span>64.43</span><span class="c2">%, a sensitivity of </span><span>59.49</span><span class="c2">%, a specificity of </span><span>69.38</span><span class="c2">%, and a Kappa value of 28.</span><span>87</span><span class="c6 c2">%. These results were very similar to that of the logistic regression model.</span></p><h2 class="c0" id="h.r9abvf4si9y6"><span class="c6 c13">Support Vector Machine</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A support vector machine is an algorithm that finds a hyperplane in an N (N = number of features) dimensional space that is typically used to classify data. Linear SVMs consider the most important training points to be support vectors because they define the hyperplane. We used the linear SVM for this data.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the resulting model, the accuracy was lower than that of previous models at just </span><span>63.66</span><span class="c2">%. The sensitivity was </span><span>74.91</span><span class="c2">%, but the specificity was </span><span>52.39</span><span class="c2">%. </span><span>T</span><span class="c2">he Kappa value was </span><span>27.30%, suggesting similar performance to that of the previous model.</span></p><h2 class="c0" id="h.hyf4tc6mkmll"><span class="c6 c13">Decision Tree</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A classification tree is a supervised learning algorithm that creates partitions between a number of variables in the data to construct a tree that will classify each data point based on how they fall into the criteria of the tree. These are easy to read and interpret, so it was clear that integrating such a tree into the project would be beneficial.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The resulting tree produced average results. The accuracy was 6</span><span>2.79</span><span class="c2">%, the sensitivity was 6</span><span>1.29</span><span class="c2">%, the specificity was 6</span><span>4.29</span><span class="c2">%, and the Kappa value was </span><span>25.57</span><span class="c2">%. These were very similar to the results of the </span><span>previous </span><span class="c6 c2">models. </span></p><h2 class="c0" id="h.dnv7a4dqjxii"><span class="c6 c13">Bagging: Random Forest</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A random forest builds on the decision tree approach by creating a set of many decision trees and aggregating the result of each decision tree to come to a final decision. Random forest uses a technique known as bagging or bootstrap aggregating, where each decision tree in the forest is trained on a different subset of the original training set. Introducing this diversity amongst the trees improves the performance of the model.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The random forest trained on our dataset provided results similar to the previously discussed models. It achieved an accuracy of 64.11%, with a sensitivity of 68.11% and a specificity of 60.13%. Additionally, the Kappa value was 28.23%.</span></p><h2 class="c0" id="h.gh1tmxj53pmi"><span class="c6 c13">Boosting: AdaBoost</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AdaBoost is another tree-based model. It creates a forest of trees, but rather than bagging, it uses a technique called boosting. Trees are created one at a time in the boosting process. Each tree is evaluated on its performance, and misclassified records are given more weight in subsequent trees in order to minimize weaknesses. Instead of creating a random forest, AdaBoost creates a forest that is continuously improving its skill.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our AdaBoost model was one of the models that took the longest to complete training. It achieved an accuracy of 64.22% on the training set, with a sensitivity of 65.66% and a specificity of 62.79%. The Kappa value was 28.44%. These results remain very similar to the results achieved in the models above.</span></p><h2 class="c0" id="h.4y7lhohg4pkw"><span class="c6 c13">Boosting: XGBoost</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XGBoost is our final tree-based model in this report. XGBoost stands for Extreme Gradient Boosting, which attempts to minimize a loss function with each tree that is built. In this way, it is similar to neural networks, which also attempt to minimize a loss function to improve the accuracy of the fitted model.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our XGBoost model also performed similarly to our other models. It achieved an accuracy of 61.44%, with a sensitivity of 53.53% and a specificity of 69.28%. The Kappa value was 22.81%.</span></p><h2 class="c0" id="h.k125y6ly2xdr"><span class="c6 c13">Neural Network</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The neural network was the final model we trained in this project. Neural networks rely on layers of interconnected neurons to identify patterns in datasets and produce an output. The training process involves the modification of weights and biases for each neuron as training data is passed through, the output is evaluated, and then the adjustments are made by the backpropagation algorithm. Neural networks maximize their skill by minimizing a loss function through a process known as gradient descent.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once again, our neural network showed similar skill to the other models discussed above. The accuracy was 64.48%, with a sensitivity of 69.40% and a specificity of 59.57%. The Kappa value was 28.97%.</span></p><h2 class="c0" id="h.jkibbnijr3nw"><span class="c6 c13">Model Comparison and Summary</span></h2><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As we&rsquo;ve discussed, all of our models performed with a similar level of accuracy. All accuracies were between 60% and 65% on the test set. Sensitivity and specificity were largely between 50% and 70%, and the Kappa values were consistently between 20% and 30%. The table below shows the results of the eight fitted models from the confusion matrix on the test set.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A fitted binary classification model should have an accuracy greater than 50% since 50% accuracy is equivalent to a random guess. Our models consistently perform better than a random guess, classifying a record correctly 60% to 65% of the time. However, that certainly leaves room for improvement. Additionally, the fact that the more advanced models did not outperform a simple model, like logistic regression, suggests that the more advanced models were unable to learn any complex patterns in the dataset.</span></p><a id="t.dae2dcd4f342102f3c14b074b49c2be9033e8716"></a><a id="t.0"></a><table class="c11"><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1 c20"><span class="c6 c2"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">Accuracy</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">Sensitivity</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">Specificity</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">Kappa</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Logistic Regression</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6437</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6979</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.5895</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2874</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Linear Discriminant Analysis</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6443</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.5949</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6938</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2887</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Support Vector Machine</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6366</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.7491</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.5239</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2730</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Decision Tree</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6279</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6129</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6429</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2557</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Random Forest</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6411</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6811</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6013</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2823</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">AdaBoost</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6422</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6566</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6279</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2844</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">XGBoost</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6144</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.5353</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6928</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2281</span></p></td></tr><tr class="c3"><td class="c10" colspan="1" rowspan="1"><p class="c1"><span class="c6 c2">Neural Network</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6448</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.6940</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.5957</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c4"><span class="c6 c2">0.2897</span></p></td></tr></table><p class="c9 c20"><span class="c6 c2"></span></p><h1 class="c0" id="h.vkpneg8kg7on"><span class="c19">Conclusion</span></h1><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The main result of this project was that we demonstrated the ability to correctly classify a loan applicant as having a loan charged off or fully paid around 60% to 65% of the time. A significant amount of data exploration and preprocessing was undertaken to best prepare the data for model fitting. We found that there was little difference between the performance of simple models, like logistic regression, and more complex models, such as a neural network.</span></p><p class="c9"><span class="c6 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Due to the large size of the data, a major issue we faced was the ability to train the models on a normal sample size of the data. We originally tried to partition the data into 70% training data, but many of the models took well over 10 minutes to complete, so we had to terminate running these models and adjust the sample size. This led us to use just over 1,500 records to train the models in order to produce a model that could be trained within a few minutes of running. This has obvious limitations to the skill of the models. Training on more records may have improved the outcomes of the models.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We also had limited knowledge of the topic being studied</span><span>:</span><span class="c2">&nbsp;loan repayment. While some of the variables were easy to understand</span><span>, </span><span class="c2">such as annual income and term, others were very technical</span><span>, </span><span class="c2">such as revolving line utilization rate</span><span>,</span><span class="c2">&nbsp;and we weren&rsquo;t sure how important they would be for prediction. Thus, we had to rely heavily on </span><span>our </span><span class="c2">data exploration for such variables to tell us how closely correlated and potentially significant these features were.</span><span>&nbsp;It is also possible that we missed interactions between variables that would have been useful.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are a few possibilities for future work. One could be incorporating the &lsquo;purpose&rsquo; variable; we ended up dropping this variable because including it would have required us to create </span><span>several </span><span class="c2">dummy variables to </span><span>represent the 12 unique values. We believed all the additional features would not be worth the marginal additional information it would provide to the model</span><span class="c2">. </span><span>Had we investigated further,</span><span class="c2">&nbsp;this variable could have </span><span>been </span><span class="c2">potentially </span><span>helpful for</span><span class="c2">&nbsp;our models and </span><span>for </span><span class="c6 c2">predicting the response variable.</span></p><p class="c9"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another potential possibility for future work is incorporating more of the techniques we learned this semester. One such topic is the prediction of rare events. Given that the data provided was quite unbalanced, with only about 20% of the data containing instances of &ldquo;Charged off&rdquo; or unpaid loans, the topics covered in this lesson could have been beneficial. We</span><span>&nbsp;used a subset of the dataset to solve our problem with the unbalanced dataset. However, there were other techniques discussed this semester that could have been more helpful. </span><span class="c2">This includes testing different cutoff rates and comparing undersampling versus oversampling to create more accurate models with less imbalance.</span><hr style="page-break-before:always;display:none;"></p><h1 class="c0" id="h.rnw3w75vp2ey"><span class="c19">Works Cited</span></h1><p class="c9 c21"><span>Forbes. &quot;Lending Club.&quot; </span><span class="c25">Forbes</span><span class="c6 c2">, www.forbes.com/companies/lending-club/?sh=3f0e46a0e4c3. Accessed 1 May 2023.</span></p><p class="c9 c21"><span>Kagan, Julia. &quot;What Is Peer-to-Peer (P2P) Lending? Definition and How It Works.&quot; </span><span class="c25">Investopedia</span><span class="c6 c2">, Dotdash Meredith, 9 Feb. 2023, www.investopedia.com/terms/p/peer-to-peer-lending.asp. Accessed 1 May 2023.</span></p><p class="c9 c20"><span class="c6 c2"></span></p></body></html>